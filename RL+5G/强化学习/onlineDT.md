# online decision transformer

## Abstract

离线强化学习问题可表述为序列建模问题，与大规模语言建模问题类似。提出了ODT，基于序列建模的强化学习算法，将离线预训练和在线微调融合在一个统一的框架中。

## Introduction

序列建模存在生成预训练的情况，并已经扩展到了强化学习领域，将离线强化学习转换成监督学习问题。但这种监督学习范式很难确定能否扩展到在线环境进行强化学习。强化学习的在线微调涉及探索，需要通过探索获取数据，这导致离线RL的传统监督学习目标在在线环境中不足。此外，在线算法访问离线数据会对在线性能产生零甚至负面影响。

文章引入了ODT，该框架建立在DT架构的基础上，适合在线交互成本高昂、需要离线预训练和样本高效微调的场景。纠正了几个DT与在线学习不兼容的问题，pipeline性能更卓越。

在线阶段，勘探目标时，将确定性政策转为随机性政策。使用策略熵来量化探索，但ODT的策略熵在轨迹的总体水平受限，并且其对偶形式规范了监督学习目标。

接下来，开发了一种重放缓冲区，缓冲区存储轨迹并通过ODT在线部署进行填充。同时将事后经验重放的概念融入到ODT中，并在增强之前使用更正的返回标记重新标记推出的轨迹，这样可以避免在线部署期间指定的所需回报的策略与部署期间观察到的真实回报不匹配。

## Related Work

## Preliminaries

将环境建模为马尔可夫过程

### Decision Transformer

DT将轨迹处理为3种输入标记的序列：RTG、state和actions。

DT通过GPT架构对策略进行参数化

## Online Decision Transformer

训练数据的限制，纯离线数据集上训练的强化学习策略通常不是最优的，自然就出现了通过在线交互来微调预训练的RL代理。文章对决策转换器进行修改，实现样本高效的在线微调。

第一步：提出了一个广义的概率学习目标，对其进行扩展来解释ODT的探索。