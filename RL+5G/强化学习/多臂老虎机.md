强化学习与其他机器学习最大的区别是：
强化学习关注**评价性反馈**，而不是**指导性反馈**。
- 评价性反馈：关注每一步action的好坏程度
- 指导性反馈：得知最优action（多用于监督学习）
# A k-armed Bandit Problem
k臂老虎机
- 每次在k个选项中做出一个选择，叫做一个action；根据action反馈一个奖励值，奖励值服从一个特定的概率分布（该分布需要通过学习来寻找）
- 目标是另收获的奖励累计值最大化
问题的公式描述：
$$q_{*}(a) {\doteq} {\mathbb{E}}[R_{t}|A_{t}=a]$$
- $A_{t}$：第t步做出的action
- $R_{t}$：第t步行动后得到的回报值
- $a$：任意的行动
- $q_{*}(a)$：行动a的理论期望值
玩家一开始是不知道$q_{*}(a)$的情况的，所以要建立一套对所有a的评价体系，根据目前拥有的知识，来估计/猜测当前第t步a的回报值$Q_{t}(a){\approx}q_{*}(a)$。评价体系的建立是最关键的。
假设已经建立好了一套评价体系，那么玩家该如何根据**评价性反馈**去行动呢？这时就提到两个概念：
- 利用（Exploiting）：采取贪心行动，即根据目前已掌握的信息来做当前最优选择。
- 探索（Exploring）：放弃贪心行动，去探索潜在的，有长远价值的信息。
“利用”是利用玩家当前已掌握知识内的最佳策略，确保回报玩家认知范围内的最佳策略值。
“探索”是试错，去尝试一些信息量少的action，但action信息量少不代表一定低回报。
# Action-value Method
