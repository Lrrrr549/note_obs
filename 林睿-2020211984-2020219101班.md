大模型任务：ChatGLM系列
论文列表：
[1]Zeng, Aohan and Liu, Xiao and Du, Zhengxiao and Wang, Zihan and Lai, Hanyu and Ding, Ming and Yang, Zhuoyi and Xu, Yifan and Zheng, Wendi and Xia, Xiao and others.2022.[GLM-130B: An Open Bilingual Pre-trained Model](). arXiv e-prints
[2]Zhengxiao Du, Yujie Qian, Xiao Liu, Ming Ding, Jiezhong Qiu, Zhilin Yang, and Jie Tang. 2022. [GLM: General Language Model Pretraining with Autoregressive Blank Infilling](https://aclanthology.org/2022.acl-long.26). In _Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)_, pages 320–335, Dublin, Ireland. Association for Computational Linguistics.

[[2103.10360] GLM: General Language Model Pretraining with Autoregressive Blank Infilling (arxiv.org)](https://arxiv.org/abs/2103.10360)